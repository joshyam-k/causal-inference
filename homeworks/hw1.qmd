---
title: "hw1"
format: pdf
pdf-engine: xelatex
mainfont: Times
editor: visual
---

## 1

Prove each of the following. You may use the results from previous parts of the problem in further proofs. For each proof, I have indicated where you should start. For the purposes of these proofs here, assume that X, Y , and Z are continuous random variables. Assume that a, b and c are constants.

### a

$$
\begin{aligned}
E[aX + bY + c] &= \int\int (ax + by + c)p(x,y)dx dy \\
&= \int\int axp(x,y)dx dy + \int\int byp(x,y)dx dy + \int\int c p(x,y)dx dy \\
&= a\int x\bigg[\int p(x,y) dy \bigg] dx + b\int y\bigg[\int p(x,y) dx \bigg] dy + c\int\int p(x,y) dx dy \\
&= a\int x p(x) dx + b\int yp(y) dy + c\cdot 1 \\
&= aE[X] + bE[Y] + c
\end{aligned}
$$

### b

$$
\begin{aligned}
cov(X,Y) &= E\Big[(X - E[X])(Y- E[Y])\Big] \\
&= E\Big[XY - XE[Y] - YE[X] + E[Y]E[X]\Big] \\
&= E[XY] - E[XE[Y]] - E[YE[X]] + E[E[Y]E[X]] \\
&= E[XY] - E[Y]E[X] - E[X]E[Y] + E[Y]E[X]E[1] \qquad \text{since E[X] and E[Y] are just constants} \\
&= E[XY] - E[X]E[Y]
\end{aligned}
$$

### c

$$
\begin{aligned}
E[Y|X = x] &= \int y p(y| X = x)dy \\
&= \int y\frac{p(y,x)}{p(x)}dy \\
&= \int \int y\frac{p(y, x, z)}{p(x)} dzdy \qquad \text{reverse marginalization} \\
&= \int \int y\frac{p(y | x, z)p(x,z)}{p(x)}dzdy \qquad \text{bayes rule} \\
&= \int \frac{p(x,z)}{p(x)} \bigg[\int yp(y|x,z)dy\bigg]dz \qquad \text{separating terms} \\
&= \int p(z | x) E[Y | X = x, Z = z] dz \\ 
&= E\Big[E[Y | X, Z] \ \Big| \  X = x\Big] 
\end{aligned}
$$

The transition between the second to last and the last step relies on the fact that since we are conditioning on $X = x$, E\[Y\|X,Z\] is just a function of Z, meaning that the outer expectation is over $p(z | x)$.

### d

$$
\begin{aligned}
E[h(X,Y)] &= \int\int h(x, y)p(x, y)dx dy \\
&= \int\int h(x,y)p(y | x)p(x)dxdy \qquad \text{bayes rule} \\
&= \int p(x)\bigg[\int h(x, y)p(y|x)dy \bigg]dx \qquad \text{rearranging terms} \\
&= \int p(x)E[h(x,y) | X = x] \qquad \text{since we condition on x, E[h(x,y)|x] is over p(y|x)} \\
&= E\bigg[E[h(X, Y) | X = x]\bigg] \qquad \text{this outer expectation is just over p(x)}
\end{aligned}
$$

### e

$$
\begin{aligned}
Var(Y) &= E[Y^2] - E[Y]^2 \\
&= E\Big[E[Y^2 | X]\Big] - E[Y]^2 \\
&= E\Big[Var(Y|X) + E[Y|X]^2\Big] - E\Big[E[Y|X]\Big]^2 \\
&= E[Var(Y|X)] + E\Big[E[Y|X]^2\Big] - E\Big[E[Y|X]\Big]^2 \\
&= E[Var(Y|X)] + Var(E[Y|X]) \qquad \text{defintion of variance}
\end{aligned}
$$

## 2

Prove each of the following statements. In each, let $X_{(n)}$ and $Y_{(n)}$ be sequences of random variables, and let c and d be constants.

### a

Choose some $\varepsilon > 0$. We can write.

$$
P\bigg(\Big|(X_{(n)} + Y_{(n)}) - (c + d)\Big| \ge \varepsilon\bigg) = P\bigg(\Big|(X_{(n)} -c) + (Y_{(n)} - d)\Big| \ge \varepsilon\bigg)
$$

But if the event $\Big|(X_{(n)} -c) + (Y_{(n)} - d)\Big| \ge \varepsilon$ happened then the event

-   "$|X_{(n)} - c| \ge \varepsilon/2$ or $|Y_{(n)} - d| \ge \varepsilon/2$" necessarily happened as well

But because the first event happening means necessarily that the second happened, then we can say that the first event is a subset of the second event. Of course if $A \subseteq B$ then $P(A) \le P(B)$ so we can say that

$$
P\bigg(\Big|(X_{(n)} + Y_{(n)}) - (c + d)\Big| \ge \varepsilon\bigg) \le P\bigg(\Big|X_{(n)} - c\Big| \ge \varepsilon/2 \ \bigcup \ \Big|Y_{(n)} - d\Big| \ge \varepsilon/2\bigg)
$$ 
We also know that $P(A \cup B) \le P(A) + P(B)$ so

$$
\begin{aligned}
P\bigg(\Big|(X_{(n)} + Y_{(n)}) - (c + d)\Big| \ge \varepsilon\bigg) &\le P\bigg(\Big|X_{(n)} - c\Big| \ge \varepsilon/2 \ \bigcup \ \Big|Y_{(n)} - d\Big| \ge \varepsilon/2\bigg) \\
&\le P\bigg(\Big|X_{(n)} - c\Big| \ge \varepsilon/2 \bigg) + P\bigg(\Big|Y_{(n)} - d\Big| \ge \varepsilon/2 \bigg)
\end{aligned}
$$

But since we know that $X_{(n)}\xrightarrow{p}c$ and $Y_{(n)}\xrightarrow{p}d$ then we know that 

$$
P\bigg(\Big|X_{(n)} - c\Big| \ge \varepsilon/2 \bigg) + P\bigg(\Big|Y_{(n)} - d\Big| \ge \varepsilon/2 \bigg) \rightarrow 0\qquad \text{as }n\to\infty
$$

Importantly these are sequences of numbers and not random variables so we can indeed say that since each term individually converges to zero, then so does their sum. We are not using what we were trying to prove.

But now what we have is that for any $\varepsilon > 0$,

$$
\lim_{n\to\infty} P\bigg(\Big|(X_{(n)} + Y_{(n)}) - (c + d)\Big| \ge \varepsilon\bigg) = 0
$$
which is exactly what it means for $X_{(n)} + Y_{(n)}\xrightarrow{p}c + d$

## 3

We simply apply Chebyshev's inequality to $X_{(n)}$, choosing any $\varepsilon$, to get

$$
P\bigg(\Big|X_{(n)} - E[X_{(n)}]\Big| \ge \varepsilon\bigg) \le \frac{var(X_{(n)})}{\varepsilon^2}
$$

And since we know that $var(X_{(n)})\rightarrow 0$, then we know that 

$$
\frac{1}{\varepsilon^2}\cdot var(X_{(n)}) \rightarrow \frac{1}{\varepsilon^2}\cdot 0 = 0
$$

Of course probabilities are bounded below by zero, so we get that the probability expression is squeezed towards zero as $n$ increases. And since we also know that $E[X_{(n)}] \rightarrow c$, then when we take the limit of the above expression we get

$$
\lim_{n\to\infty} P\bigg(\Big|X_{(n)} - c\Big| \ge \varepsilon\bigg)  = 0
$$

which is exactly what it means for $X_{(n)}\xrightarrow{p}c$

## 4

We know that $\sqrt{n}(\hat{\theta} - \theta) \xrightarrow{d} N(0, \sigma^2)$ and $\hat{\sigma} \xrightarrow{p}\sigma$ and we can write

$$
\frac{\sqrt{n}(\hat{\theta} - \theta)}{\hat{\sigma}} = \frac{\sigma}{\hat{\sigma}}\frac{\sqrt{n}(\hat{\theta} - \theta)}{\sigma}
$$

Next we apply the continuous mapping theorem we can say that

$$
\frac{\sigma}{\hat{\sigma}} \xrightarrow{p} \frac{\sigma}{\sigma} \qquad \text{and} \qquad \frac{\sqrt{n}(\hat{\theta} - \theta)}{\sigma} \xrightarrow{d} N(0, 1)
$$

Next Slutsky's theorem tells us that

$$
\frac{\sqrt{n}(\hat{\theta} - \theta)}{\hat{\sigma}} = \frac{\sigma}{\hat{\sigma}}\frac{\sqrt{n}(\hat{\theta} - \theta)}{\sigma} \xrightarrow{d} 1\cdot N(0, 1)
$$

And by definition of convergence in distribution we get that

$$
\lim_{n\to\infty}P\bigg(-z_{1-\alpha /2} \le \frac{\sqrt{n}(\hat{\theta} - \theta)}{\hat{\sigma}} \le z_{1-\alpha/2} \bigg) = 1- \alpha
$$

We can then do some rearranging within the parentheses

$$
\begin{aligned}
-z_{1-\alpha /2} \le \frac{\sqrt{n}(\hat{\theta} - \theta)}{\hat{\sigma}} \le z_{1-\alpha/2} &= -z_{1-\alpha /2}\frac{\hat{\sigma}}{\sqrt{n}} \le \hat{\theta} - \theta \le z_{1-\alpha/2}\frac{\hat{\sigma}}{\sqrt{n}} \\
&=\hat{\theta} - z_{1-\alpha /2}\frac{\hat{\sigma}}{\sqrt{n}} \le -\theta \le \hat{\theta} + z_{1-\alpha /2}\frac{\hat{\sigma}}{\sqrt{n}} \\
&= \hat{\theta} - z_{1-\alpha /2}\frac{\hat{\sigma}}{\sqrt{n}} \le \theta \le \hat{\theta} + z_{1-\alpha /2}\frac{\hat{\sigma}}{\sqrt{n}} \qquad \text{symmetry of normal dist}
\end{aligned}
$$

so we have that

$$
\lim_{n\to\infty}P\bigg(\hat{\theta} - z_{1-\alpha /2}\frac{\hat{\sigma}}{\sqrt{n}} \le \theta \le \hat{\theta} + z_{1-\alpha /2}\frac{\hat{\sigma}}{\sqrt{n}}\bigg) = 1 - \alpha
$$

as desired.

## 5

### a

Well in OLS we choose $\beta$ such that $\mathbf{X}^T(\mathbf{Y} - \mathbf{X}\beta) = 0$ ($\hat{\beta}_{OLS}$ solves this equation).

We can write this out as follows

$$
 \begin{pmatrix}
    1 & \dots & 1 \\
    X_{1,1} & \dots & X_{n,1} \\
    \vdots & \ddots & \vdots \\
    X_{1,k-1} & \dots & X_{n,k-1} 
    \end{pmatrix}
    \begin{pmatrix}
    e_1 \\ e_2 \\ \vdots \\ e_n
    \end{pmatrix} =
    \begin{pmatrix}
    0 \\ 0 \\ \vdots \\ 0
    \end{pmatrix}
$$

but immediately we can see that an immediate consequence of this is that

$$
1\cdot e_1 + 1\cdot e_2 + ... + 1\cdot e_n = \sum_{i=1}^ne_i = 0
$$

so of course $\frac{1}{n}\sum e_i = 0$ as well.

### b

## 6

```{r, message=F, warning=F}
library(tidyverse)
library(patchwork)
library(moderndive)

set.seed(1)
n <- 1000
x <- rnorm(n)
epsilon <- (x^2 + 1) * (rchisq(n, df=1)-1)
y <- 1 + x + epsilon

df <- tibble(
  x = x,
  y = y,
  eps = epsilon
)
```


### a

```{r}
mod <- lm(y ~ x, df)

df_preds <- df %>% 
  mutate(
    prediction = predict(mod, newx = x),
    residual = y - prediction
    ) 
```

```{r, warning = F, message = F}
## linearity
p1 <- df_preds %>% 
  ggplot(aes(x, y)) +
  geom_point() +
  geom_smooth(method = "lm", se = F) +
  theme_bw()

p2 <- df_preds %>% 
  ggplot(aes(x = prediction, y = residual)) +
  geom_point() +
  geom_hline(yintercept = 0, color = "orange", linetype = 2, linewidth = 1) +
  theme_bw()

p3 <- df_preds %>% 
  ggplot(aes(x = residual)) +
  geom_histogram(fill = "midnightblue", color = "white") +
  theme_bw()
  

(p1 + p2) / p3
```


### b

Linearity

$$
\begin{aligned}
E[\varepsilon | X ] &= E[(X^2 + 1)\cdot(\chi^2_{1} - 1) | X ] \\
&= E[(X^2\cdot\chi^2_{1} + \chi^2_{1} - X^2 - 1 | X ] 
\end{aligned}
$$



### c

#### i

```{r}
get_regression_table(mod) %>% 
  filter(term == "x") %>% 
  select(term, lower_ci, upper_ci) 
```
    
#### ii

```{r}
set.seed(100)
df_list <- list()
for(i in 1:1000){
  data <- df %>% 
    slice_sample(n = 1000, replace = T)
  
  df_list[[i]] <- data
}

get_coef <- function(data) {
  mod <- lm(y ~ x, data)
  return(coef(summary(mod))[2,1])
}

boot_coefs <- df_list %>% 
  map_dbl(.f = get_coef)

quantile(boot_coefs, c(0.025, 0.975))
```


#### iii

```{r}
coef(summary(mod))[2, 1] + c(-1, 1)*qnorm(0.975)*sd(boot_coefs)
```

#### iv

```{r}
library(sandwich)

varBeta <- vcovHC(mod, type="HC")
se <- sqrt(diag(varBeta))[2]

coef(summary(mod))[2, 1] + c(-1, 1)*qnorm(0.975)*se
```



### d

